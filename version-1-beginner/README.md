# E2E Data Engineering Project â€” Version 1 (Beginner)

This is **Version 1 (Beginner)** of my end-to-end data engineering project using **open-source tools only**.  
It demonstrates how to build a simple data pipeline using **PostgreSQL and Python**, with Docker for easy setup.

---

## ðŸ—‚ Project Structure

version-1-beginner/
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ users.csv
â”‚ â”œâ”€â”€ products.csv
â”‚ â””â”€â”€ orders.csv
â”œâ”€â”€ scripts/
â”‚ â””â”€â”€ load_data.py
â””â”€â”€ README.md

- **data/**: Contains sample CSV files used in this version  
- **scripts/**: Python scripts to create tables and load CSV data into PostgreSQL  

---

## ðŸ›  Tools Used

- **Docker & Docker Compose** â€” for containerized PostgreSQL and Python environment  
- **PostgreSQL** â€” relational database  
- **Python** â€” for scripting the ETL (Extract, Transform, Load) pipeline  
- **Pandas** â€” for reading and processing CSV files  
- **psycopg2** â€” PostgreSQL adapter for Python  

---

## ðŸš€ How to Run

1. **Clone the repository**

```bash
git clone https://github.com/Tharmithan-N/e2e-data-engineering-project.git
cd e2e-data-engineering-project
```


2. **Start Docker containers**

```bash
docker-compose up -d
```

3. **Enter Python container**

```bash
docker exec -it de_python bash
```

4. **Install dependencies (inside container)**

```bash
pip install psycopg2-binary pandas
```

5. **Run the ETL script**

```bash
python scripts/load_data.py
```

6. **You should see output like:**

Tables created successfully!
Data loaded successfully!
Users rows: 5
Products rows: 5
Orders rows: 5
